---
title: "Week 7: Hypothesis Testing, Decision Errors, & Multiple Comparisons"
#format: html
format: pdf
#format: docx
editor: visual
execute: 
  echo: false
  message: false
  warning: false
---

```{r setup}
#| include: false

library(tidyverse)
library(mosaic)
library(here)
library(pander)
library(broom)
```

```{r data-cleaning}
#| include: false

movies <- readxl::read_xlsx(here::here("two_days", 
                              "Week 2 - Viz & Summarize Quantitative Variables",
                              "activity",
                              "quantitative EDA",
                              "data",
                              "movies_2020.xlsx"
                              )
                   ) |>  
  distinct(Movie, .keep_all = TRUE)

title_ids <- read_csv(here::here("two_days", 
                               "Week 2 - Viz & Summarize Quantitative Variables",
                               "activity",
                               "quantitative EDA",
                               "data",
                               "movie_ids.csv")
                      )
  
movie_ids <- left_join(movies, 
                       title_ids, 
                       by = 
                         intersect(
                           colnames(movies), 
                           colnames(title_ids)
                           )
                       )

ratings <- read_csv(here::here("two_days", 
                               "Week 2 - Viz & Summarize Quantitative Variables",
                               "activity",
                               "quantitative EDA",
                               "data",
                               "ratings.csv")
                      )

movie_ratings <- left_join(movie_ids, 
                           ratings, 
                           by = "id"
                           ) |> 
  select(Movie, 
         Genre, 
         `2020 Gross`, 
         runtimeMinutes, 
         averageRating, 
         numVotes) |> 
  drop_na(averageRating)

genres <- tibble(Genre = c("Comedy", 
                           "Documentary", "Drama", "Horror", 
                           "Thriller/Suspense")
                 )

movie_ratings <- movie_ratings |> 
  semi_join(genres, by = "Genre")
```


## Inference for Many Means (ANOVA)

Alright, so we just learned about how we can analyze the differences in **many** means using ANOVA (**AN**alysis **O**f **VA**riance). As a refresher, with an
ANOVA, we're comparing the variability *within* groups (MSE) to the variability
*between* groups (MSG). 

If we believe that the mean of at least one group is different from the others, 
ideally in a visualization we'd like to see:

- large differences in the means **between** the groups
- small amounts of variability **within** each group

\vspace{0.25in}

1. Sketch an example of three boxplots that exhibit the characteristics above. 

\newpage

## Hypotheses in an ANOVA

In an ANOVA, we only do hypothesis testing (no confidence intervals until after
ANOVA), and the hypotheses are always the same:

$$
H_0: \mu_1 = \mu_2 = \dots = \mu_k
$$

$$
H_A: \text{At least one of the means } (\mu_k) \text{ is different}
$$

\vspace{0.25cm}

Let's refresh what we saw for the differences in `averageRating` between the
`Genre`s.

\vspace{0.25cm}

```{r favstats}
favstats(averageRating ~ Genre, 
         data = movie_ratings) |> 
  pander()
```

\vspace{0.25cm}

2. How many groups do we have in our ANOVA? 

\vspace{0.25in}

3. Rewrite the null an alternative hypotheses above to reflect the number of 
groups in our analysis. *It would be nice to know what groups the means correspond with!*

\newpage

## Visualizing an ANOVA

By plotting the data **before** we do a hypothesis test, we get a better 
understanding of *why* we got a small / medium / large p-value! 

Here are faceted histograms visualizing the distribution of movie ratings across
the different genres. 

```{r movie-histograms}
#| out-width: 80%
#| layout-nrow: 1

ggplot(data = movie_ratings, 
       mapping = aes(x = averageRating)
       ) +
  geom_histogram(binwidth = 1) + 
  facet_wrap(~ Genre, scales = "free") + 
  labs(x = "Average Movie Rating (from IMDb)") 
```

\vspace{0.25cm}

4. How different are the centers of these groups from each other? 

\vspace{0.5in}

5. How different are the spreads of these groups from each other?

\vspace{0.5in}

You might like to have the side-by-side boxplots to answer #4, so 
here you go! 

```{r movie-boxplots}
#| out-width: 60%

ggplot(data = movie_ratings, 
       mapping = aes(x = averageRating, 
                     y = Genre)
       ) +
  geom_boxplot() + 
  labs(x = "Average Movie Rating (from IMDb)", 
       title = "Genre of Movie", 
       y = "IMDb Moving Ratings by Movie Genre") 
```


6. Overall, do you believe any of the genres stand out as **really** different
from the others?

\vspace{1in}

## Conditions of an ANOVA

Like every statistical analysis we've done in this class, when conducting an 
ANOVA you have two types of methods, (1) a simulation-based method or (2) a 
theory-based (mathematical) method. The book describes both options, but today
we are going to focus on **theory-based** methods. 

In an ANOVA there are two conditions that we need to evaluation regardless of 
which method we use:

- independence of observations within **and** between groups
- equal variance across every group

7. Evaluate if you believe the independence condition is or is not violated. 
Keep in mind that there are **two** components to this condition you need to 
discuss! 

\newpage

8. Evaluate if you believe the equal variance condition is or is not violated. 
Make specific reference to the visualizations and the summary statistics 
presented previously!

\vspace{1in}

__Additional Condition for Theory-Based Methods:__

As we have seen before, with a theory-based method we have one additional 
condition:

> nearly normal distributions across every group

\vspace{0.25cm}

9. Why should we use faceted histograms to assess this condition rather than 
side-by-side boxplots? 

\vspace{0.5in}

10. Evaluate if you believe the normality condition is or is not violated. Make
specific reference to the visualization you stated in #8!

\vspace{0.5in}

## Carrying Out an ANOVA in `R`

Now that we've checked the conditions of an ANOVA, we are ready to perform the
analysis! Earlier today, you were introduced to the `aov()` function. The 
`aov()` function is the tool we use to perform an ANOVA in `R`. 

11. Fill in the necessary components of the code below. 

```{r aov-code, eval = FALSE}
aov(_______________________________ ~ __________________________,
    data = movie_ratings)
```

Alright, if we ran the code you just input, we'd get the following table:

\vspace{0.25cm}

```{r aov-output}
#| echo: false

model <- aov(averageRating ~ Genre, data = movie_ratings) |> 
  tidy()

pander(model)
```

\vspace{0.25cm}

12. What is the mean squares of `Genre`?

\vspace{0.25in}

13. What is the mean squares of the errors? 

\vspace{0.25in}

14. How was the `statistic` of `r round(model$statistic[1], digits = 3)` found?
What is the name of that statistic?

\vspace{0.5in}

15. What is the p-value associated with that statistic?

\vspace{0.25in}

16. Based on the p-value, at an $\alpha = 0.05$ significance level, what 
decision would you reach regarding your hypotheses?

\vspace{0.25in}

17. Based on your decision in #15, what would you conclude regarding the 
mean movie rating across these genres?

\vspace{0.5in}

\newpage 

Let's revisit the statistics we first saw. It's entirely possible that in #5 you
said that you didn't believe there were "substantial" difference across 
these groups. 

```{r favstats-again}
#| echo: false
favstats(averageRating ~ Genre, 
         data = movie_ratings)
```

18. How does this connect with the p-value you obtained in #14?

\vspace{1in}

## Hypothesis Testing Errors

In a hypothesis test, there are two competing hypotheses: the null and the
alternative. We make a statement about which one might be true, but we might 
choose incorrectly. There are four possible scenarios in a hypothesis test:

|                      | $H_0$ is True  | $H_0$ is False  |
|----------------------|----------------|-----------------|
| Reject $H_0$         | Type I Error   | Good Decision!  |
| Fail to Reject $H_0$ | Good Decision! | Type II Error   |

19. Based on the decision you reached in #15, what type of error could you have
made?

\vspace{0.25in}

20. With an $\alpha = 0.05$, what percent of the time would we expect to make a
Type I error?

\vspace{0.25in}

21. How does $\alpha$ relate to the probability of making a Type II error?

\vspace{0.5in}

## Inference after ANOVA

If we had found a "significant" p-value, we could have concluded that at least one
of the genres had a different mean movie rating. However, an ANOVA **does not**
tell us which group(s) is(are) driving the differences. 

What we could do is compare all possible combinations of two means. With five 
groups, that would result in 10 different hypothesis tests for a difference in
means (e.g., $\mu_{\text{Comedy}} - \mu_{\text{Documentary}}$, 
$\mu_{\text{Comedy}} - \mu_{\text{Drama}}$, 
$\mu_{\text{Horror}} - \mu_{\text{Thriller}}$, etc.). 

However, for each hypothesis test we do at an $\alpha$ of 0.05, we risk making a
Type I error 5% of the time. In fact, we can make a mathematical equation for 
the probability of making a Type I Error, based on the number of tests we 
perform. 

\vspace{0.25cm}

$$
\text{Probability of Making a Type I Error} = 1 - \text{Probability of Not Making a Type I Error}
$$
\vspace{-0.2cm}
$$
\text{Probability of Making a Type I Error} = 1 - (0.95)^{\text{\# of tests}}
$$
\vspace{0.2cm}

22. If we do 10 hypothesis tests, what is the probability of us making a Type 
I Error?

\vspace{0.5in}

## Remedy to Type I Error Inflation

```{r bonferroni}
#| out-width: 10%
#| fig-align: left

knitr::include_graphics("images/Bonferroni.jpg")
```

One solution to the problem of multiple comparisons is called the Bonferroni correction. Essentially, you take your $\alpha$ threshold and divide it by the number of tests you are going to perform. This is referred to as $\alpha^*$. 

You then use this $\alpha^*$ value as the new threshold value for **every** 
pairwise comparison. If a comparison's p-value is less than $\alpha^*$, then 
you reject $H_0$. If a comparison's p-value is greater than $\alpha^*$, then 
you fail to reject $H_0$

23. If our original $\alpha$ was 0.05, what value should we use for $\alpha^*$?

\vspace{0.5in}

## Post-Hoc Comparisons

Below is a table of all 10 of the hypothesis tests we could do when comparing
the means of two groups. 

24. Using the $\alpha^*$ you found in #22, circle the hypothesis tests whose 
p-values are less than $\alpha^*$. 

```{r post-hoc, echo = FALSE}
#| echo: false

pairwise.t.test(x = movie_ratings$averageRating, 
                g = movie_ratings$Genre,
  p.adjust.method = "none") |>  
  tidy() |> 
  rename(`Group 1` = group1, 
         `Group 2` = group2, 
         `p-value` = p.value) %>% 
  pander()
```

Your $\alpha^*$ value should be much less than your original $\alpha$ of 0.05, which makes it **harder** to reject the null. 

25. When the number of comparisons gets larger, what happens to the probability of making a Type II error? 




---
title: "Week 3 Day 2: Sleepless Nights"
output: html_document
---

```{r, include = FALSE}
library(tidyverse)
library(mosaic)

sleep_hours <- read_csv("sleep_hours.csv")
```

![](sheep.jpg)

Last class, we explored utilizing a bootstrap distribution to obtain a 
confidence interval for the population mean. There is another approach we could
have used instead, which focuses on mathematical formulas and not simulation. 

These "theory-based" mathematical formulas have a similar idea: 

> Obtain a distribution of statistics we might have expected from other
> samples.

However, instead of using simulation to obtain these "new" samples / resamples, 
instead we will use the $t$-distribution. 

### Central Limit Theorem (CLT)

This key theorem in Statistics say that when we collect a "sufficiently large" sample of $n$ **independent** observations from a population with mean $\mu$
and standard deviation $\sigma,$ we know the **sampling distribution** of 
$\bar{x}$ will be nearly Normal with mean $\mu$ and standard deviation 
$\frac{\sigma}{\sqrt{n}}$. 

In order for us to feel confident that we can use the CLT with our data, we 
need to check two conditions:

- Independence of Observations
- Normality

__1. Do you believe that the 50 observations collected in this sample are 
independent? Why or why not?__

</br>
</br>
</br>

__2. Based on the histogram from yesterday's activity, do you believe it is
safe to say that the distribution of hours slept is approximately Normal? Why
or why not?__

</br>
</br>
</br>



### The $t$-distribution

![](Gosset.jpg)

The $t$-distribution became well known in 1908, in a paper in *Biometrika* 
published by William Sealey Gosset. Gosset published the paper under the
pseudonym "Student," which is why you sometimes hear the distribution called 
"Student's t". Gosset worked at the Guinness Brewery in Dublin, Ireland, and
was interested in the problems of small samples [(Wikipedia article)](https://en.wikipedia.org/wiki/Student%27s_t-distribution#History_and_etymology). 

The $t$-distribution is always centered at zero and has a single parameter: **degrees of freedom**. The degrees of freedom describe exactly what the shape 
of the $t$-distribution looks like. 

We will use a $t$-distribution with $n - 1$ degrees of freedom to model the sample mean. When we have more observations, the degrees of freedom will be
larger and the $t$-distribution will look more like the Normal distribution. 

__3. How many degrees of freedom will we use for our $t$-distribution?__

</br>
</br>

__4. Compared to a $t$-distribution with 20 degrees of freedom, will your 
distribution have *more* or *less* area in the tails?__

</br>
</br>
</br>

The CLT says if we have a "large" sample of independent observations and don't
have any outliers, then we know the sampling distribution has a standard
deviation of $\frac{\sigma}{\sqrt{n}}.$ But, we don't usually know the value 
of $\sigma$, since it is the **population** standard deviation. So, instead 
we substitute in $s$, the sample standard deviation: $\frac{s}{\sqrt{n}}$.

</br>

```{r}
favstats(~ hours, data = sleep_hours)
```

__5. Given the summary statistics above, calculate the estimated standard
deviation of the sampling distribution (standard error).__

</br>
</br>

__6. Is this value larger or smaller than the standard deviation in your
sample? Why do you believe this is the case?__

</br>
</br>
</br>

Did you notice that $\frac{\sigma}{\sqrt{n}}$ did not equal $s$? This is
because the variability between **individuals’** number of hours slept is 
**VERY** different from the variability between the **average** number of hours slept across samples of people.

> Key Idea:
> There will be less sample-to-sample variability than in person-to-person 
> variablity!

### Using the $t$-distribution to create a confidence interval

__17. There is one requirement for using the SE method to construct a 
confidence interval. What is this requirement? Can we safely use the SE method 
to construct a confidence 


```{r, echo = FALSE}
sleep_hours %>%
  specify(response = hours) %>% 
  hypothesize(null = "point", mu = 8) %>% 
  assume(distribution = "t") %>% 
  visualize() +
  labs(x = "t-statistic", 
       y = "Density", 
       title = "Theory-Based Null Distribution")

```


Do you think a 90% confidence interval be wider or narrower than your 95%
confidence interval? Explain.


Which part of the confidence interval is changing?

- Statistic (midpoint)
- Multiplier
- Standard error

How does the multiplier change from the 95% to the 90% confidence interval? 

- Multiplier is larger
- Multiplier is smaller
- Multiplier stays the same

How would the midpoint change for a 99% confidence interval compared to the 90% interval?

How would the margin of error change for a 99% confidence interval compared to
the 90% interval? Explain.


### Comparison with Previous Results

__What confidence interval did you obtain yesterday? Is it similar to or
different from the interval you obtained today?__

</br>
</br>
</br>

###

<!-- t-test for mu_0 -->

### Generalizability

16.	Think again about how the sample was selected from the population. Do you
feel comfortable generalizing the results of your analysis to the population of
all STAT 218 students at your school? Explain.

17. How would the 95% confidence interval change if you surveyed a much smaller
number of students? Assume that the sample mean would still be 5.795.



### Conclusions 

It’s important to keep in mind that these conditions are rough guidelines and
not a guarantee! All theory-based methods are approximations which work best when the distributions are symmetric, when sample sizes are large, and when there are
no large outliers. When in doubt, use a simulation-based method as a
cross-check! If the two methods give very different results you should consult
a statistician!

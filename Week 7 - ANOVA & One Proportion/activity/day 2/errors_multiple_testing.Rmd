---
title: "Week 7 Day 2: Hypothesis Testing, Errors, & Multiple Comparisons"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## General Social Survey -- Past Analysis 

```{r data-load, include = FALSE}
GSS <- read_csv(here::here("Week 7 - ANOVA & One Proportion", 
                           "activity", 
                           "day 2", 
                           "data", 
                           "GSS_clean.csv")
)
```

We've worked with data from the General Social Survey (GSS) for a few different
analyses in this class. Specifically, we used these data to test if there was 
evidence that the true mean number of hours worked in a week for working 
Americans was different than 40. 

Today, we are going to look at a differnt version of this question. We will 
investigate if there is evidence that the average number of hours worked per
week differs for at least one marital status groups. 


## Inference for Many Means (ANOVA)

We've seen how to do inference for a single mean and for a difference of means.
Now, we'll study how to do inference on **many** means. To do this, we'll use a
procedure called ANOVA for **AN**alysis **O**f **VA**riance. With an ANOVA, 
we're comparing the variability *within* groups to the variability *between*
groups. 

If we believe that the mean of at least one group is different from the others, 
ideally in a visualization we'd like to see:

- large differences in the means **between** the groups
- small amounts of variability **within** each group

\vspace{0.25in}

1. Sketch an example of three boxplots that exhibit the characteristics above. 

\newpage

## Hypotheses in an ANOVA

In an ANOVA, we only do hypothesis testing (no confidence intervals until after
ANOVA), and the hypotheses are always the same:

$$
H_0: \mu_1 = \mu_2 = \dots = \mu_k \\
H_A: \text{At least one of the means is different}
$$

Let's consider the variables `marital_status` and
`number_of_hours_worked_last_week`. To know how many groups there are in the
`marital_status` variable, we can use our friend `favstats()`!

```{r favstats}
favstats(number_of_hours_worked_last_week ~ marital_status, 
         data = GSS)
```

\vspace{0.25cm}

1. How many groups do we have here? 

\vspace{0.25in}

2. Rewrite the null an alternative hypotheses above to reflect the number of 
groups in our analysis. 

\newpage

## Visualizing an ANOVA

By plotting the data **before** we do a hypothesis test, we get a better 
understanding of *why* we got a low / medium / high p-value! 

Here are side-by-side boxplots, visualizing the distribution of hours worked
last week across the different marital statuses. 

```{r gss-boxplots}
ggplot(data = GSS, 
       mapping = aes(x = number_of_hours_worked_last_week, 
                     y = marital_status)) +
  geom_boxplot() 
```


\newpage 

To do ANOVA in R, we can use the built-in R function `aov()`, which will use an F distribution as the distributional approximation for the null distribution. In order to use this distributional approximation, two conditions must be met:

- the data must be normally distributed, or all the $n_i$s must be greater than 30
- the variances of each group should be approximately equal. In practice, we check this by determining if $sd_{max}/sd_{min}<2$. 

How can we check those conditions?

```{r}

```

Now that we've ensured our conditions are met, we can use the `aov()` command. Much like the `lm()` command, it works best if you save the result from `aov()` into a named R object. 

Try running the following code. What happens in your Environment?

```{r}
a1 <- aov(number_of_hours_worked_last_week ~ marital_status, data = GSS)
```

Now, let's run `summary()` on that object,

```{r}
summary(a1)
```

What are the degrees of freedom for the groups? The degrees of freedom for the error? The total degrees of freedom?

What is the test statistic? What is the p-value?

What is our generic conclusion at the $\alpha=0.05$ level?

## Inference after ANOVA

Because we found a significant p-value and concluded that at least one of the means is different, we can do inference after ANOVA. This is similar to inference on a single mean or a difference of means, but we use the square root of the MSE to estimate $\sigma$ instead of using the sample standard deviation. For example, for a confidence interval for the difference of two means, the standard error would be

$$
\sqrt{MSE\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}
$$
Let's consider the difference between the number of hours worked per week by divorced people and married people. How would we estimate the standard error? 

```{r}
sqrt(208.6 * (1 / 403 + 1 / 998))
```

To do inference after ANOVA, we use a t-distribution with the degrees of freedom for the error. What is that in this example? 

The easiest way to do pairwise tests in R is to do them all at once. You may recall from lecture that doing many tests at once can lead to a problem of multiple comparisons. 

One solution to the problem of multiple comparisons is called the Bonferroni correction, which is where you essentially divide your $\alpha$ cutoff number by the number of tests you are going to run. But, there are other, more sophisticated methods. One of these is called Tukey's Honest Significant Difference, which we will use here. 

Let's create an 80\% confidence interval for the difference between the number of hours worked by divorced and married people. 

```{r}
TukeyHSD(a1, conf.level = 0.8)
```

How can we interpret the interval? 

Now, let's consider the hypothesis tests. Which ones showed significance? 
---
title: "Do more beautiful professors get better course evaluations?"
subtitle: "Example Project Writeup"
author: "by Dr. Theobold"
output: html_document
---

```{r setup, include = FALSE}
# This is code to make your report format look nice! 
# Please don't modify these options! 
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      fig.align = "center")

library(tidyverse)
library(mosaic)
library(moderndive)
library(knitr)

```

```{r data}
library(openintro)

evals_ind <- evals %>% 
  group_by(prof_id) %>% 
  sample_n(size = 1, replace = FALSE)
```

![Google search results for "attractive professors"](attractive_professors.png)

## Data Description

Today, most colleges require the teaching of faculty to be evaluated through 
anonymous student evaluations. Historically, student evaluations date back to 
the 1920s, where faculty were in favor of this form of evaluation replacing the
casual judgments of lone administrators. Faculty were largely dissatisfied with
the alternatives, and eventually most accepted the premise of student
evaluations. Professors today, however, look a lot different than professors in
the 1920s or even the 1970s. 

The data for this project come from end of semester student evaluations for a
large sample of professors from The University of Texas at Austin. The original
data set published in *Beauty in the Classroom* (Hamermesh & Parker, 2005) are
available through the **openintro** R package. 

I am interested in these data because I have read numerous papers on how 
teaching evaluations to be biased against women and people of color. Many of
these papers, similar to the *Beauty in the Classroom* article, propose that 
student evaluations often reflect biases in favor of non-teaching related
characteristics, such as the physical appearance of the instructor. These data
allow for me to explore the relationship between professor's attractiveness and
their teaching evaluations. 

For this project, I will focus on two variables: the average attractiveness of 
the professor and the professor's average course evaluation score. The study had
six students rate the attractiveness of each professor based on their picture 
from 1 (lowest) to 10 (highest). The average of these six scores is the variable
used for this project. 

## Data Visualization
<!-- In this section you create three visualizations of your data -->

### Histogram of Average Teaching Evalution Scores

```{r histogram-1}
evals_ind %>% 
  ggplot(mapping = aes(x = score)
         ) +
  geom_histogram(binwidth = 0.37) + 
  labs(x = "Average Course Evaluation Score (from all student responses)", 
       y = "Number of Professors")

```

In the histogram of course evaluation scores we see that the most frequent
scores are around 4.5. The lowest scores these professors received was a 2 
and the highest scores was a 5. The distribution is unimodal and left skewed,
with most of the professors receiving scores over 3.5. Since the scores range
from 0 to 5, this distribution suggests that, overall, students rate these 
professors favorably. 

### Histogram of Average Attractiveness Scores

```{r histogram-2}
evals_ind %>% 
  ggplot(mapping = aes(x = bty_avg)
         ) +
  geom_histogram(binwidth = 0.7) + 
  labs(x = "Average Attractiveness (Scored by 6 Students)", 
       y = "Number of Professors")

```

In the histogram of attractiveness scores, we see a unimodal distribution with a
slight right skew. The majority of professors received an average attractiveness
score of about 4. There were a few professors who received low attractiveness 
scores of about 2, but I would not consider these outliers as they are not far
from the center of the distribution. However, there are professors who received
average attractiveness scores of over 7.5, which I would classify as potential 
outliers, as they are quite far from the center of the distribution.

### Scatterplot of Avearge Beauty Score and Average Course Evalution Score

```{r scatterplot}
evals_ind %>% 
  ggplot(mapping = aes(x = bty_avg, 
                       y = score)
         ) +
  geom_jitter() + 
  geom_smooth(method = "lm") +
  labs(x = "Average Attractiveness (Scored by 6 Students)", 
       y = "Average Course Evaluation Score (from all student responses)")

```

In the scattterplot we see a positive relationship between professor's
attractiveness and their course evaluation score. The relationship is weak to 
moderate, as there is a great deal of variability in course evaluation scores 
across all levels of attractiveness. This variability makes it more difficult 
to say the shape of the relationship, but I would say there is a weak linear
relationship between these two variables. Finally, there are a few outliers, 
namely the professors with course evaluation scores under 3 and attractiveness
scores below 2. These are both far from the center (high leverage) and far from
the regression line, which suggests they may be quite influential. 


## Linear Relationship Between Average Course Evaluation Scores and Average
Beauty Score

### Model Conditions

There are four conditions required to be verified before we perform inference
on the linear relationship between these two variables. These conditions are:
linearity of the relationship, independence of observations, nearly normal
residuals, and constant variability. 

__Linearity:__ As seen in the scatterplot above, the relationship between 
average course evaluation score and average beauty score is weakly linear. 
Although the relationship is not very strong, it does not appear that there is
evidence of a non-linear (e.g., quadratic, logarithmic) relationship.

__Independence:__ These data were from a convenience sample of professors at 
the University of Texas at Austin. The researchers obtained the course
evaluation scores for every professor in the sample, for every course they 
taught. Thus, for some professors there were multiple observations. 

Having multiple observations for each professor violates the independence
condition. Thus, for each professor I randomly selected **one** of their
courses. After removing these repeated observations, I no longer believe the
independence condition is violated. There is no reason to believe that knowing 
the evaluation score of one professor should give me perfect information about 
another professor's evaluation score. 

__Normality of Residuals:__ The condition of normality can be checked using 
either the distribution of the responses (average evaluation scores) or the 
distribution of residuals from the regression model. Based on the histogram 
above, it does not appear that the condition of nearly normal responses is 
violated. While the distribution has a slight left skew, the sample size is 94
professors, well over the threshold of 60 for substantially skewed
distributions.

__Equal Variance:__ The variability of the evaluation scores appears similar 
across the regression line. While there is a larger spread in the evaluation 
scores for professors with beauty ratings between 4 and 5, the size of these
differences is not large. Instead, across all beauty ratings, it appears that 
course evaluation scores vary from about 3 to about 5. 

Since I have not seen any major violations of these conditions, I believe using 
t-based methods will not incorrectly estimate a p-value or confidence interval. 

### Coefficient Estimates

```{r coefficients}

eval_lm <- lm(score ~ bty_avg, data = evals_ind)

## Code to get coefficient table from linear regression
get_regression_table(eval_lm, conf.level = 0.90) %>% 
  kable()
```

From the output above, we can see that the estimated intercept for the
regression line is 3.594 and the estimated slope is 0.084. The y-intercept of 
3.594 represents the predicted mean evaluation score for a professor who 
received an average beauty score of zero. The lowest average beauty score
observed in these data was two, so I believe this would be considered
extrapolation. 

The regression line suggests that for every increase of one in average beauty 
score, we expect the mean course evaluation score to increase by about 0.084.
Course evaluation scores are on a scale of 0 to 5, so an increase of 0.084 is
quite small. 

### Inference for the Slope

<!-- Here is where you describe the decision for the hypothesis test for whether the slope is 0.  -->

Although the size of the slope coefficient is small (0.086), at a 0.05 
significance level I conclude that the relationship between average beauty 
score and average evaluation score is "significantly" different from
0 (t-stat = 2.356, p-value = 0.021). 

The 95% confidence interval is (0.025, 0.147), which implies that I am 95%
confident that for an increase of one in average beauty score I should expect an
increase between 0.025 and 0.147 in the average course evaluation score. 
